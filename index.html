<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Annapurna</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<h1>Annapurna</h1>
						<p>I work on human-computer interaction (HCI) that helps people solve problems.</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#intro" class="active">About Me</a></li>
							<li><a href="#projects">Projects</a></li> 
							<li><a href="#education">Education</a></li>
							<li><a href="#contact">Contact</a></li>
							<!--li><a href="#second">Teaching</a></li>
							<li><a href="#cta">Outreach</a></li-->
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

							<section id="intro" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>About Me</h2>
										</header>
										<p>I am a current Ph.D. student in Cognitive Science at the UCSD Design Lab studying human-computer interaction (HCI). I am interested in how we can use HCI to both improve and understand the cognitive processes that learners use for solving problems that are new to them. I am advised by Profs. Steven Dow and Philip Guo. </p>
								</div>
							</section>

							<section id="projects" class="main">
								<header class="major">
									<h2>Projects</h2>
								</header>

								<h2>Pedagogy, Scaffolding, and Outcomes in Introductory CS With LLMs</h2>
								<p>Large language models (LLMs) are being used increasingly in computer science in general, and classrooms in particular. Plugins such as Github Copilot leverage LLMs and provide code suggestions in real time, and it is unclear how this impacts students' learning of fundamental computing principles. Our objective is to compare student outcomes in an LLM-augmented introductory computer science class (CS1-LLM) as compared with a traditional introductory computer science class (CS1), specifically examining differential outcomes across demographic backgrounds. We are developing a novel introductory CS course led by Prof. Leo Porter, UCSD, and Prof. Dan Zingaro, University of Toronto. A key component of CS1-LLM is its instruction on the use of LLMs (specifically Github Copilot) to help write effective code, including guidance with the basics of prompt engineering. A second key property of CS1-LLM is its accelerated pace. Our course assumes that students will use Copilot to solve typical CS1 problems quickly. It therefore covers basic programming topics that typically span an entire course (e.g. variables, conditionals, loops, and functions) in the first few weeks. The freed course time will be spent on skills such as problem decomposition, testing, and debugging that are not traditionally taught in CS1 courses5. Students in CS1 have formerly needed to acquire these skills without explicit instruction; such student-discovered knowledge is referred to as “hidden curriculum”, and it can be a barrier to students from underrepresented backgrounds.</p>
								We aim to answer the following questions: <br>
									<b>RQ1</b>: How do student outcomes in CS1-LLM compare with traditional CS1? <br>
									<b>RQ2</b>: How do the differences in outcomes across demographics in CS1-LLM compare to traditional CS1?<br><br>
								<p>We are collecting data from surveys, assessments, and student-consented recorded coding sessions. We will gather data both between courses (CS1 from Fall 2022 vs. the current Fall 2023 CS1-LLM) and within CS1-LLM. From 2022, we have data about student demographics, perceptions (through surveys), grades, and assessment performance. We will collect analogous data from the current CS1-LLM offering. Among the programming skills assessed in both courses (e.g. use of conditionals, loops, and functions), we will analyze performance differences. We will also analyze how outcomes and attitudes for students from underrepresented backgrounds differ between CS1 and CS1-LLM as compared to those from highly-represented backgrounds. Within the course, we will examine survey and assessment performance data to conduct similar analyses about how students of different backgrounds fare in CS1-LLM. We will compare this to the within-course analysis of the 2022 offering of CS1.</p>

								<h2>Explicit Principles and Explanations in Learning Novel Reasoning Tasks </h2>
								<p>How do people algorithmically tackle hard problems and develop generalizable principles from individual problem instances? When faced with a novel situation or task, many humans are able to formulate abstract principles that are useful to guide problem solving. The strength of this ability varies between individuals. Previous studies have investigated the role of self-explanation in understanding a novel task or phenomenon. One aspect of self-explanation that has been identified as a possible source of utility is the learner's generation of an explicit abstract principle that can then be applied to future situations or problems. We are interested in each of these factors (self explanation and reasoning experience) separately, as well as their interaction. Existing research on self-explanation has shown its importance, but the question of how exactly it helps in understanding remains open, with explicit abstract principle generation being one possible way that it contributes to understanding. Our research questions are: </p>
								<b>RQ1</b>: When people learn to solve a novel task, do they formulate explicit principles during their learning of it?<br>
								<b>RQ2</b>: Does prior formal reasoning experience influence the ability to learn a novel task and the ability to formulate these explicit principles? <br><br>
								<p>To investigate these questions, I am working with Jay McClelland in the Parallel Distributed Processing lab to examine how people learn and understand a problem-solving task that is new to them. Our experiment arises from observations in Nam and McClelland's recent investigation of participants' learning of a particular strategy in Sudoku, a popular puzzle game (Nam & McClelland, 2021). Their study as well as our current one both examine participants' learning of the Hidden Single strategy, which is a Sudoku solving technique that can be applied to certain partially-filled grids. A Sudoku player can use the Hidden Single strategy to identify a digit that must go in a particular cell of the grid when specific conditions of known and unknown digits in the rows, columns, and boxes are met. In our experiment, Sudoku-naive participants on the Prolific platform (a human subject pool similar to Mechanical Turk) are given a series of increasingly difficult Sudoku grids with sufficiently many digits to constrain a highlighted cell, and asked to fill it in with the correct digit. We provide unlimited attempts for each puzzle and feedback of correctness only, with no explanation or further instructions. Participants can use their feedback from each puzzle to learn the constraints of Sudoku, and apply these constraints to the progressively harder puzzles. We then ask them to explain their solving process in a series of carefully designed questions. We plan to analyze whether their explanations contained instance-specific references (e.g. "I knew that it couldn't be a 7 because there was another 7 in the row") or generalizable, abstract references (e.g. "I knew it couldn't be any digit that was already in the row, column, or 3x3 box"). To develop the puzzle sequence and questions, we carefully consider the scaffolding of constraints learned in each puzzle with the goal of observing how rule-like principles might emerge. We hypothesize that participants with more formal education will correctly solve more puzzles, and be more inclined and better able to create abstract explanations. </p>

								<h2>Lightweight Teaching Interventions in Tutor Training </h2>

								<p>In academia, a variety of people are required to teach students whether or not they have a background in teaching. There are many classes in which novice TAs and undergraduate course assistants are expected to lead both classroom discussions and one-on-one interactions with students with no prior experience. Often, this results in subpar instructional results such as explanations that students do not understand or the provision of answers themselves rather than explanations that help students learn. While rigorous teaching programs exist for instructional careers, these are not required or often feasible for TAs and tutors. A more lightweight teaching intervention is therefore needed for training TAs and tutors. Many universities have created TA training courses (e.g. CSE 599 at UCSD), but there is not yet a standard for such courses and a concrete set of teaching principles that are expected to be mastered. </p>
								<p>The need for such interventions closely mirrors a problem I encountered as an educator prior to my experience as a graduate student. As a co-founder of Transform Tutoring, a company that provides one-on-one tutoring for high school students, I led the interview process for STEM tutors. Interviews were mock tutoring sessions wherein the candidate would play a tutor and the interviewer would play a student. I noticed that almost all candidates (mostly upper undergraduate or graduate students) simply explained a topic, or even solution to a question, from start to finish without engaging the student. Many candidates were clearly knowledgeable in their domain, and I believed that if they guided in effective teaching, the quality of their tutoring improved drastically. I developed a required pre-interview reading for candidates which included an explanation and example of student-driven learning--that is, allowing students to find the next step of a problem on their own rather than providing it to them. My <a href = "https://drive.google.com/file/d/1NlNz230Fs-VGpq-fdleqBDb7bzkULTzi/view?usp=sharing"> student-driven learning </a> guide yielded much better interviews, and illustrated to me the promise of lightweight teaching interviews. As a graduate student, I adapted this guide to create an <a href = "https://drive.google.com/file/d/1RaDsJ8RtEBtF8-5U89Ets0G52L0jcRJJ/view?usp=drive_link"> Introduction to Python TA Training</a> that has been used by the Cognitive Science department for training new TAs and tutors. I have run several TA trainings and am developing a TA training course that has concrete learning outcomes for TAs and tutors. As I develop these trainings along with teaching professors in my department, we will aim to answer the following questions:</p>
								<b>RQ1</b>: There are many well-established teaching principles in pedagogical literature. Which ones are suited to lightweight teaching interventions that could be effectively taught to TAs and tutors without prior teaching training? <br>
								<b>RQ1</b>: Which teaching principles taught via lightweight teaching interventions are most effective for improving student understanding? <br><br>
								By answering these questions, we hope to develop trainings that are both <b>feasible to teach TAs and tutors</b> and <b>effective for improving student learning outcomes</b>. 

							</section>

						<!-- Education -->
							<section id="education" class="main">
								<header class="major">
									<h2>Education</h2>
								</header>
								<h3>Current: UC San Diego, Ph.D. student in Cognitive Science</h3>
								<h3>Undergraduate: Johns Hopkins University, B.S. Neuroscience, B.S. Computer Science </h3>

								<a href = "https://drive.google.com/file/d/1mZ_jnPzqztNA7fzENRYjiBvNwz3FKe0q/view?usp=sharing">View my full CV here. </a>


							</section>

						<!-- Get Started -->
							<section id="contact" class="main">
								<header class="major">
									<h2>Contact & Socials</h2> 
									<a href="mailto:avadaparty@ucsd.edu">Email</a> | 
									<a href = "https://linkedin.com/in/annapurna-vadaparty"> LinkedIn</a>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">&copy; Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>